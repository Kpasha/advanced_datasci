{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29a0e87d5fe7e00f9bfec060019c25beab67035e"
   },
   "source": [
    "This Python 3 environment comes with many helpful analytics libraries installed\n",
    "It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "For example, here's several helpful packages to load in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "da163d263ba61685860530a25d26fa8282f04c42",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d3399023ba73a368703f69c8c68a789188cb6be"
   },
   "source": [
    "Input data files are available in the \"../input/\" directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac2f694b-60f6-4053-b919-93eb96f97e79",
    "_uuid": "464b8ff121e74a14770413ead7b3bc5e0cb7c4e7"
   },
   "source": [
    "Any results I write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Import Libraries and Data:\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training file\n",
    "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "print(train.head())\n",
    "print('---------------------')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Target Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_errors = train['logerror']\n",
    "upper_lim = np.percentile(log_errors, 99.5)\n",
    "lower_lim = np.percentile(log_errors, 0.5)\n",
    "log_errors = log_errors.clip(lower=lower_lim, upper=upper_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.hist(log_errors, bins=300)\n",
    "plt.title('Distribution of Target Variable (log-error)')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('log-error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-errors are close to normally distributed around a 0 mean, but with a slightly positive skew. There are also a considerable number of outliers, I will explore whether removing these improves model performance.\n",
    "\n",
    "Proportion of Missing Values in Each Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load property features/description file\n",
    "prop = pd.read_csv(\"../input/properties_2016.csv\")\n",
    "print(prop.head())\n",
    "print('---------------------')\n",
    "print(prop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = prop.drop('parcelid', axis=1).isnull().sum()\n",
    "nans.sort_values(ascending=True, inplace=True)\n",
    "nans = nans / prop.shape[0]\n",
    "#print(nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(len(nans.index)), nans.values)\n",
    "plt.xticks(range(len(nans.index)), nans.index.values, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns which have a very high proportion of missing values. It may be worth analysing these more closely.\n",
    "\n",
    "Monthly Effects on Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['transaction_month'] = pd.DatetimeIndex(train['transactiondate']).month\n",
    "train.sort_values('transaction_month', axis=0, ascending=True, inplace=True)\n",
    "print(train.head())\n",
    "\n",
    "ax = sns.stripplot(x=train['transaction_month'], y=train['logerror'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submission we are required to predict values for October, November and December. The differing distributions of the target variable over these months indicates that it may be useful to create an additional 'transaction_month' feature as shown above. Lets have a closer look at the distribution across only October, November and December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.stripplot(x=train['transaction_month'][train['transaction_month'] > 9], y=train['logerror'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of Transactions in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = train['transaction_month'].value_counts(normalize=True)\n",
    "trans = pd.DataFrame(trans)\n",
    "trans['month'] = trans.index\n",
    "trans = trans.sort_values('month', ascending=True)\n",
    "trans.set_index('month')\n",
    "trans.rename({'transaction_month' : ''})\n",
    "print(trans)\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(months)), trans['transaction_month'])\n",
    "plt.title('Proportion of Transactions per Month')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(range(len(months)), months, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datase contains more transactions occuring in the Spring and Summer months, although it must be noted that some transactions from October, November and December have been removed to form the competition's test set (thanks to nonrandom for pointing this out).\n",
    "\n",
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaN values with -1 and encode object columns \n",
    "for x in prop.columns:\n",
    "    prop[x] = prop[x].fillna(-1)\n",
    "\n",
    "#many more parcelids in properties file, merge with training file\n",
    "train = pd.merge(train, prop, on='parcelid', how='left')\n",
    "print(train.head())\n",
    "print('---------------------')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in train[['transactiondate', 'hashottuborspa', 'propertycountylandusecode', 'propertyzoningdesc', 'fireplaceflag', 'taxdelinquencyflag']]:\n",
    "    label = LabelEncoder()\n",
    "    label.fit(list(train[c].values))\n",
    "    train[c] = label.transform(list(train[c].values))\n",
    "\n",
    "x_train = train.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "y_train = train['logerror']\n",
    "\n",
    "print(x_train.head())\n",
    "print('------------')\n",
    "print(y_train.head())\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=30, max_features=None)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "rf_importance = rf.feature_importances_\n",
    "\n",
    "\n",
    "importance = pd.DataFrame()\n",
    "importance['features'] = x_train.columns\n",
    "importance['importance'] = rf_importance\n",
    "print(importance.head())\n",
    "importance.sort_values('importance', axis=0, inplace=True, ascending=False)\n",
    "print('------------')\n",
    "print(importance.head())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.bar(range(len(importance)), importance['importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Name')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(importance)), importance['features'], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the greatest importance in predicting the log-error comes from features involving taxes and geographical location of the property. Notably, the 'transaction_month' feature that was engineered earlier was the 12th most important feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3d7b5d3a7eebbb2acf1a63ba447555fc4564b92"
   },
   "source": [
    "  Importing Libraries or Packages that are needed throughout the Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d605750d2ca0f24eb951546fac7e29ee8e7ccbef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  import xgboost as  xgb \n",
    "  import random \n",
    "  import datetime as dt \n",
    "  import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1818712cc96ba417f2e3b8d7a68d72bff94fffe9"
   },
   "source": [
    "# Load the Datasets\n",
    "\n",
    "We need to load the datasets that will be needed to train our machine learning algorithms, handle our data and make predictions.\n",
    "Note:  that these datasets are the ones that are already provided once you enter the competition by accepting terms and conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1485bf3901e3dcf2f4f9990835e4753c5df216bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "     train = pd.read_csv('../input/train_2016_v2.csv' , parse_dates=[\"transactiondate\"]) \n",
    "     properties = pd.read_csv('../input/properties_2016.csv')   \n",
    "     test = pd.read_csv('../input/sample_submission.csv') \n",
    "     test= test.rename(columns={'ParcelId': 'parcelid'}) \n",
    "    #To make it easier for merging datasets on same column_id later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf7a436d379d1f63db49b2fd2a260f4ed09da745"
   },
   "source": [
    "# Analyse the Dimensions of our Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6314bd84da81374a346b39a476f4adfbb45091f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:(90275, 3)\n",
      "Property Size:(2985217, 58)\n",
      "Sample Size:(2985217, 7)\n"
     ]
    }
   ],
   "source": [
    "     print(\"Training Size:\" + str(train.shape))\n",
    "     print(\"Property Size:\" + str(properties.shape))\n",
    "     print(\"Sample Size:\" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ac5907ee6038164ee75c4c0fb4e73901c4d1aca"
   },
   "source": [
    "# Type Converting the DataSet \n",
    "\n",
    "The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64.\n",
    "\n",
    "Therefore, in order to make sure that all of our columns types are in\n",
    "float32, I am implementing the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "fe6c9bb7d406209bbbef76ec3133bf810a402a58",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " for c, dtype in zip(properties.columns, properties.dtypes):\n",
    "     if dtype == np.float64:        \n",
    "         properties[c] = properties[c].astype(np.float32)\n",
    "     if dtype == np.int64:\n",
    "         properties[c] = properties[c].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "867d374a22eb507331ac498c9828a0442a679268",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " for column in test.columns:\n",
    "     if test[column].dtype == int:\n",
    "         test[column] = test[column].astype(np.int32)\n",
    "     if test[column].dtype == float:\n",
    "         test[column] = test[column].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72a936957dd5503ff2d06eb7e11c33718b134180"
   },
   "source": [
    "### living area proportions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "12c61bf65333f096478a898a2e6909557764ead2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d486bd85281dfb9cd734caa675aa52a8d74d366"
   },
   "source": [
    "### tax value ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2ad5ffafebddce3ed1255168ec76c8605057728c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ddb1153c297509e7980631219ebd46e131e02be"
   },
   "source": [
    "### tax value proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "568ca7160adc57c6d5c6b32f921f498e17e0cdc3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties['value_prop'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8170a0356ff20162f676a718217613bd94555aa"
   },
   "source": [
    "### Merging the Datasets\n",
    "\n",
    "We are merging the properties dataset with training and testing dataset for model building and testing prediction #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "d206a569a7b3c6e31fba74f3ee07f46a68b2ae9d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = train.merge(properties, how='left', on='parcelid') \n",
    "df_test = test.merge(properties, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c66e656ae6dfc469ec0c3fe48bc1d2de9a84e1b8"
   },
   "source": [
    "### Remove previous variables to keep some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b8ffc435be693e19905431fe4d70f4c501f87c55",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del properties, train\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d76a85d3f510765ebd1a2771889be65c824cb0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage reduction...\n"
     ]
    }
   ],
   "source": [
    "print('Memory usage reduction...')\n",
    "df_train[['latitude', 'longitude']] /= 1e6\n",
    "df_test[['latitude', 'longitude']] /= 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "063ff5f96294e5f70ffd746c31999f4cf20ff6d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['censustractandblock'] /= 1e12\n",
    "df_test['censustractandblock'] /= 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcf7d78888ae623b54c2b8a67a058c75e6c78951"
   },
   "source": [
    "# Label Encoding For Machine Learning &amp; Filling Missing Values \n",
    "\n",
    "We are now label encoding our datasets. All of the machine learning algorithms employed in scikit learn assume that the data being fed to them is in numerical form. LabelEncoding ensures that all of our categorical variables are in numerical representation. Also note that we are filling the missing values in our dataset with a zero before label encoding them. This is to ensure that label encoder function does not experience any problems while carrying out its operation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "99e0fdf0393c36e183cbf4b770b1b32277aa4bf9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "lbl = LabelEncoder()\n",
    "for c in df_train.columns:\n",
    "    df_train[c]=df_train[c].fillna(0)\n",
    "    if df_train[c].dtype == 'object':\n",
    "        lbl.fit(list(df_train[c].values))\n",
    "        df_train[c] = lbl.transform(list(df_train[c].values))\n",
    "\n",
    "for c in df_test.columns:\n",
    "    df_test[c]=df_test[c].fillna(0)\n",
    "    if df_test[c].dtype == 'object':\n",
    "        lbl.fit(list(df_test[c].values))\n",
    "        df_test[c] = lbl.transform(list(df_test[c].values))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b407761b48ea9fd7e011c0bf8b1c6a34b6c6760f"
   },
   "source": [
    "### Rearranging the DataSets\n",
    "\n",
    "We will now drop the features that serve no useful purpose. We will also split our data and divide it into the representation to make it clear which features are to be treated as determinants in predicting the outcome for our target feature. Make sure to include the same features in the test set as were included in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "d23a1827f7745e7879b95f745ed19769dfd6e2a5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', ], axis=1)\n",
    "\n",
    "x_test = df_test.drop(['parcelid', 'propertyzoningdesc',\n",
    "                       'propertycountylandusecode', '201610', '201611', \n",
    "                       '201612', '201710', '201711', '201712'], axis = 1) \n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = df_train['logerror'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20b963a00961c553c9a1eb027f491c47ff146a62"
   },
   "source": [
    "### Cross Validation \n",
    "\n",
    "We are dividing our datasets into the training and validation sets so that we could monitor and the test the progress of our machine learning algorithm. This would let us know when our model might be over or under fitting on the dataset that we have employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7e3f431dad4846a3ca02596e5f99c0501f1baca2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = x_train\n",
    "y = y_train \n",
    "\n",
    "Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af4b15b39c4f8b805bfd8f6a00a440bc05839162"
   },
   "source": [
    "# Implement the Xgboost\n",
    "\n",
    "We can now select the parameters for Xgboost and monitor the progress of results on our validation set. The explanation of the xgboost parameters and what they do can be found on the following link http://xgboost.readthedocs.io/en/latest/parameter.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "8a2e9220fe0d13d7d1f2d72798d2ba6e9621b23e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "dvalid = xgb.DMatrix(Xvalid, label=yvalid)\n",
    "dtest = xgb.DMatrix(x_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8b937e92503d0aed1a1a6b6799296ef24c5953b"
   },
   "source": [
    "# Try different parameters! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "db83c4989ee4ca43e8ff1921d3afb92b23a5aa2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.479871\tvalid-mae:0.480932\n",
      "Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mae hasn't improved in 100 rounds.\n",
      "[10]\ttrain-mae:0.342058\tvalid-mae:0.343132\n",
      "[20]\ttrain-mae:0.247464\tvalid-mae:0.248492\n",
      "[30]\ttrain-mae:0.183227\tvalid-mae:0.184171\n",
      "[40]\ttrain-mae:0.140217\tvalid-mae:0.141049\n",
      "[50]\ttrain-mae:0.11195\tvalid-mae:0.112601\n",
      "[60]\ttrain-mae:0.093955\tvalid-mae:0.094526\n",
      "[70]\ttrain-mae:0.082911\tvalid-mae:0.083401\n",
      "[80]\ttrain-mae:0.076417\tvalid-mae:0.07694\n",
      "[90]\ttrain-mae:0.072703\tvalid-mae:0.073257\n",
      "[100]\ttrain-mae:0.070578\tvalid-mae:0.071127\n",
      "[110]\ttrain-mae:0.069367\tvalid-mae:0.069914\n",
      "[120]\ttrain-mae:0.068667\tvalid-mae:0.069235\n",
      "[130]\ttrain-mae:0.068245\tvalid-mae:0.068853\n",
      "[140]\ttrain-mae:0.067989\tvalid-mae:0.068614\n",
      "[150]\ttrain-mae:0.067829\tvalid-mae:0.068498\n",
      "[160]\ttrain-mae:0.067721\tvalid-mae:0.068423\n",
      "[170]\ttrain-mae:0.06765\tvalid-mae:0.068381\n",
      "[180]\ttrain-mae:0.067574\tvalid-mae:0.068344\n",
      "[190]\ttrain-mae:0.067524\tvalid-mae:0.06832\n",
      "[200]\ttrain-mae:0.06749\tvalid-mae:0.068319\n",
      "[210]\ttrain-mae:0.067478\tvalid-mae:0.068338\n",
      "[220]\ttrain-mae:0.067448\tvalid-mae:0.068335\n",
      "[230]\ttrain-mae:0.067422\tvalid-mae:0.068341\n",
      "[240]\ttrain-mae:0.0674\tvalid-mae:0.068354\n",
      "[250]\ttrain-mae:0.067373\tvalid-mae:0.06837\n",
      "[260]\ttrain-mae:0.067354\tvalid-mae:0.068379\n",
      "[270]\ttrain-mae:0.067335\tvalid-mae:0.068397\n",
      "[280]\ttrain-mae:0.067316\tvalid-mae:0.068407\n",
      "[290]\ttrain-mae:0.067297\tvalid-mae:0.068423\n",
      "Stopping. Best iteration:\n",
      "[198]\ttrain-mae:0.067497\tvalid-mae:0.068315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'min_child_weight': 5, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n",
    "            'subsample': 0.85, 'lambda': 0.8, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n",
    "            'eval_metric': 'mae', 'objective': 'reg:linear' }           \n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "model_xgb = xgb.train(xgb_params, dtrain, 1000, watchlist, early_stopping_rounds=100,\n",
    "                  maximize=False, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1383c5ed9a7b198647a80cf5ca953967754585"
   },
   "source": [
    "# Predicting the results\n",
    "Let us now predict the target variable for our test dataset. All we have to do now is just fit the already trained model on the test set that we had made merging the sample file with properties dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "76483339588983e4a86186efc9a04939b1fd4a7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predicted_test_xgb = model_xgb.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e220048f299fc6f2808b86f9c1efb19e1f17a01b"
   },
   "source": [
    "# Submitting the Results \n",
    "\n",
    "Once again load the file and start submitting the results in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbe9f2c585bda2ae8e9ad1c0b90ad1f5791dd39e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_file = pd.read_csv('../input/sample_submission.csv') \n",
    "for c in sample_file.columns[sample_file.columns != 'ParcelId']:\n",
    "    sample_file[c] = Predicted_test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c3ebc4926c9ea9bf7ce47f0002cfc466f5736aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Preparing the csv file ...')\n",
    "sample_file.to_csv('xgb_predicted_results.csv', index=False, float_format='%.4f')\n",
    "print(\"Finished writing the file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
